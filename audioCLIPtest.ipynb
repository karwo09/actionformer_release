{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karolwojtulewicz/miniconda3/envs/aformer/lib/python3.10/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/karolwojtulewicz/miniconda3/envs/aformer/lib/python3.10/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/karolwojtulewicz/miniconda3/envs/aformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(f'{os.getcwd()}/AudioCLIP/audioclip')))\n",
    "from AudioCLIP.audioclip.model import AudioCLIP\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}'))\n",
    "\n",
    "from AudioCLIP.audioclip.utils.transforms import ToTensor1D\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Partial-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 3000\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'./pretrained/AudioCLIP/{MODEL_FILENAME}').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_transforms = ToTensor1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio: data/thumos/i3d_features/video_validation_0000721.wav\n",
      "spec:  (1.0372118-0.035552323j) (-1.4929607+0.7183429j)\n",
      "pow_spec:  0.0029929841 -0.15620375\n",
      "(1025, 1241)\n",
      "Processing audio: data/thumos/i3d_features/video_validation_0000332.wav\n",
      "spec:  (0.842581-0.100443274j) (-0.6222665+0.23387456j)\n",
      "pow_spec:  -0.0010245498 -0.13840768\n",
      "(1025, 707)\n",
      "Processing audio: data/thumos/i3d_features/video_validation_0000317.wav\n",
      "spec:  (0.78966635+0.16425179j) (-0.7548736-0.14949189j)\n",
      "pow_spec:  -0.00068712025 -0.115001686\n",
      "(1025, 1144)\n",
      "Processing audio: data/thumos/i3d_features/video_test_0001402.wav\n",
      "spec:  (0.38257188-0.1324005j) (-0.5068045+0.011511762j)\n",
      "pow_spec:  -0.0038254678 -0.084728256\n",
      "(1025, 774)\n",
      "Processing audio: data/thumos/i3d_features/video_validation_0000912.wav\n",
      "spec:  (1.506653+0.6517719j) (-1.4531488+0.38668558j)\n",
      "pow_spec:  0.005639918 -0.26111287\n",
      "(1025, 639)\n"
     ]
    }
   ],
   "source": [
    "paths_to_audio = glob.glob('data/thumos/i3d_features/*.wav')[:5]\n",
    "\n",
    "audio = list()\n",
    "for path_to_audio in paths_to_audio:\n",
    "    print(\"Processing audio: {}\".format(path_to_audio))\n",
    "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "    # thus, the actual time-frequency representation will be visualized\n",
    "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)).to(device))\n",
    "    spec = np.ascontiguousarray(spec.cpu().numpy()).view(np.complex64)\n",
    "    print(\"spec: \",spec.max(), spec.min())\n",
    "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "    # pow_spec = pow_spec - pow_spec.min()\n",
    "    # pow_spec = pow_spec / pow_spec.max()\n",
    "    # pow_spec = pow_spec * 255\n",
    "    pow_spec = pow_spec / np.linalg.norm(pow_spec, axis=-1, keepdims=True)\n",
    "    print(\"pow_spec: \",pow_spec.max(), pow_spec.min())\n",
    "    # pow_spec = np.abs(spec).squeeze()\n",
    "    print(pow_spec.shape)\n",
    "\n",
    "    audio.append((track, pow_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data():\n",
    "    fig, axes = plt.subplots(2, len(audio), figsize=(20, 5), dpi=100)\n",
    "\n",
    "    for idx in range(len(audio)):\n",
    "        print(f'Processing {idx + 1}/{len(audio)}')\n",
    "        track, pow_spec = audio[idx]\n",
    "\n",
    "        # draw the waveform\n",
    "        librosa.display.waveshow(track, sr=SAMPLE_RATE, ax=axes[0, idx], color='k')\n",
    "        # show the corresponding power spectrogram\n",
    "        axes[1, idx].imshow(pow_spec, origin='lower', aspect='auto', cmap='gray', vmin=-180.0, vmax=20.0)\n",
    "\n",
    "        # modify legend\n",
    "        axes[0, idx].set_title(os.path.basename(paths_to_audio[idx]))\n",
    "        axes[0, idx].set_xlabel('')\n",
    "        axes[0, idx].set_xticklabels([])\n",
    "        axes[0, idx].grid(True)\n",
    "        axes[0, idx].set_ylim(bottom=-1, top=1)\n",
    "\n",
    "        axes[1, idx].set_xlabel('Time (s)')\n",
    "        axes[1, idx].set_xticks(np.linspace(0, pow_spec.shape[1], len(axes[0, idx].get_xticks())))\n",
    "        axes[1, idx].set_xticklabels([f'{tick:.1f}' if tick == int(tick) else '' for tick in axes[0, idx].get_xticks()])\n",
    "        axes[1, idx].set_yticks(np.linspace(0, pow_spec.shape[0] - 1, 5))\n",
    "\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    axes[1, 0].set_ylabel('Filter ID')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    for idx, path in enumerate(paths_to_audio):\n",
    "        print(os.path.basename(path))\n",
    "        display(Audio(audio[idx][0], rate=SAMPLE_RATE, embed=True))\n",
    "# show_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 2048, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     audio_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([audio_transforms(track\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m _, track \u001b[39min\u001b[39;00m audio[index:index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m     ((audio_features, _, _), _), _ \u001b[39m=\u001b[39m aclp(audio\u001b[39m=\u001b[39;49maudio_)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(audio_features\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mimshow(audio_features\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/audioclip.py:151\u001b[0m, in \u001b[0;36mAudioCLIP.forward\u001b[0;34m(self, audio, image, text, batch_indices, only_embedding, flatten)\u001b[0m\n\u001b[1;32m    148\u001b[0m sample_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m audio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     audio_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_audio(audio, only_embedding, flatten)\n\u001b[1;32m    152\u001b[0m     audio_features \u001b[39m=\u001b[39m audio_features \u001b[39m/\u001b[39m audio_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m image \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/audioclip.py:121\u001b[0m, in \u001b[0;36mAudioCLIP.encode_audio\u001b[0;34m(self, audio, only_embedding, flatten)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_audio\u001b[39m(\u001b[39mself\u001b[39m, audio: torch\u001b[39m.\u001b[39mTensor, only_embedding\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, flatten\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio(audio\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice), only_embedding\u001b[39m=\u001b[39;49monly_embedding, flatten\u001b[39m=\u001b[39;49mflatten)\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/esresnet/base.py:405\u001b[0m, in \u001b[0;36mResNetWithAttention.forward\u001b[0;34m(self, x, y, skip_prepro, only_embedding, flatten)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m    404\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_last_layer_embedding(x, skip_prepro)\n\u001b[0;32m--> 405\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_reduction(x)\n\u001b[1;32m    406\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_classifier(x)\n\u001b[1;32m    408\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/esresnet/base.py:642\u001b[0m, in \u001b[0;36m_ESResNet._forward_reduction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    640\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m ch \u001b[39min\u001b[39;00m x:\n\u001b[0;32m--> 642\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(_ESResNet, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_forward_reduction(ch)\n\u001b[1;32m    643\u001b[0m     outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[1;32m    644\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(outputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/esresnet/base.py:373\u001b[0m, in \u001b[0;36mResNetWithAttention._forward_reduction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    371\u001b[0m     x_att \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    372\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m--> 373\u001b[0m     x_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt5(x_att, x\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m:])\n\u001b[1;32m    374\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m x_att\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/actionformer_release/AudioCLIP/audioclip/model/esresnet/attention.py:37\u001b[0m, in \u001b[0;36mAttention2d.forward\u001b[0;34m(self, x, size)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_depth(x)\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_point(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x)\n\u001b[1;32m     38\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[39minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[39m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aformer/lib/python3.10/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m size[i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[39mif\u001b[39;00m size_prods \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 2048, 1, 1])"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "index = 0\n",
    "for index in range(1):\n",
    "    audio_ = torch.stack([audio_transforms(track.reshape(1, -1)) for _, track in audio[index:index + 1]])\n",
    "    ((audio_features, _, _), _), _ = aclp(audio=audio_)\n",
    "    print(audio_features.shape)\n",
    "plt.imshow(audio_features.detach().cpu().squeeze().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
